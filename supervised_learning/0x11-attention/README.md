# 0x11-attention
## Learning Objectives:
1. What is the attention mechanism?
2. How to apply attention to RNNs
3. What is a transformer?
4. How to create an encoder-decoder transformer model
5. What is GPT?
6. What is BERT?
7. What is self-supervised learning?
8. How to use BERT for specific NLP tasks
9. What is SQuAD? GLUE?
