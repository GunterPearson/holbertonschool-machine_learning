# 0x03-optimization
## In this section we work on optimizing
### Learning Objectives:
1. What is a hyperparameter?
2. How and why do you normalize your input data?
3. What is a saddle point?
4. What is stochastic gradient descent?
5. What is mini-batch gradient descent?
6. What is a moving average? How do you implement it?
7. What is gradient descent with momentum? How do you implement it?
8. What is RMSProp? How do you implement it?
9. What is Adam optimization? How do you implement it?
